{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxW7yuHWHx1g"
   },
   "source": [
    "# Model Comparison: First Attempt\n",
    "\n",
    "In this notebook, three ML models:\n",
    "- NBC\n",
    "- RNN with Tokenizer\n",
    "- RNN with TextVectorization\n",
    "\n",
    "are evaluated on an unseen dataset (without labels) to assess their performance by manually checking the sentiment (human evaluation).\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 671,
     "status": "ok",
     "timestamp": 1664711205758,
     "user": {
      "displayName": "Skillcate AI",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "75gBF0VQLkwq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\hexsoftwares_ml_env\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import joblib\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts')  # add the 'scripts' directory to sys.path\n",
    "from word_normalization import preprocess_text  # for customized preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvaGTuj8H7x7"
   },
   "source": [
    "## Loading Unseen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1664711219419,
     "user": {
      "displayName": "Skillcate AI",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "y_R4RGLWLnGz",
    "outputId": "609689ae-dcc1-45dd-9e95-56ddbcff55ec",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spend your money elsewhere.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Their regular toasted bread was equally satisf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Buffet at Bellagio was far from what I ant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And the drinks are WEAK, people!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-My order was not correct.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review\n",
       "0                         Spend your money elsewhere.\n",
       "1   Their regular toasted bread was equally satisf...\n",
       "2   The Buffet at Bellagio was far from what I ant...\n",
       "3                    And the drinks are WEAK, people!\n",
       "4                          -My order was not correct.\n",
       "..                                                ...\n",
       "95  I think food should have flavor and texture an...\n",
       "96                           Appetite instantly gone.\n",
       "97  Overall I was not impressed and would not go b...\n",
       "98  The whole experience was underwhelming, and I ...\n",
       "99  Then, as if I hadn't wasted enough of my life ...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../datasets/a2_RestaurantReviews_FreshDump.tsv', delimiter = '\\t', quoting = 3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 163.47it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.dropna()\n",
    "dataset['Review'] = dataset['Review'].progress_apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  spend money elsewher\n",
       "1     regular toast bread equal satisfi occasion pat...\n",
       "2                           buffet bellagio far anticip\n",
       "3                                      drink weak peopl\n",
       "4                                     order not correct\n",
       "                            ...                        \n",
       "95                        think food flavor textur lack\n",
       "96                                 appetit instantli go\n",
       "97                 overal not impress would not go back\n",
       "98    whole experi underwhelm think well go ninja su...\n",
       "99    have not wast enough life pour salt wound draw...\n",
       "Name: Review, Length: 100, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_X = dataset.Review\n",
    "raw_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models\n",
    "\n",
    "There are two ways to use a trained model with unseen data:\n",
    "\n",
    "1. **Modular**: Import the component that performs the vectorization, apply it to the unseen data, then import the model and use it on the vectorized unseen data.\n",
    "    ```python\n",
    "    count_vectorizer = joblib.load(<vectorizer only>)\n",
    "    nbc = joblib.load(<model only>)\n",
    "    ```\n",
    "\n",
    "2. **All-in-one**: Import the object that already includes both the model and the vectorizer.\n",
    "    ```python\n",
    "    nbc = joblib.load(<GridCV that includes Pipeline that includes model and vectorizer>)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 659,
     "status": "ok",
     "timestamp": 1664711261647,
     "user": {
      "displayName": "Skillcate AI",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "HIiFYJBGNky6"
   },
   "outputs": [],
   "source": [
    "# model 1: NBC\n",
    "nbc = joblib.load('../models/sentiment_analysis_nbc_model.joblib')\n",
    "\n",
    "# model 2\n",
    "tokenizer = joblib.load(\"../text_transformers/tokenizer.pkl\")\n",
    "rnn_tokenizer = load_model(\"../models/sentiment_analysis_rnn_tokenizer_model.keras\")\n",
    "\n",
    "# model 3\n",
    "rnn_text_vectorization = load_model(\"../models/sentiment_analysis_rnn_textvectorization_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUTUUa2WLwQF"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1664711263163,
     "user": {
      "displayName": "Skillcate AI",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "xBuXLj9fOiLr",
    "outputId": "d5384d9c-91b1-4e1d-801b-b6f2cd711c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 2 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 0 2 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 2 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = nbc.predict(raw_X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['nbc_prediction'] = y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step\n",
      "[1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1\n",
      " 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      " 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "X_sequences = tokenizer.texts_to_sequences(raw_X)\n",
    "X = pad_sequences(X_sequences, maxlen=80, padding='post')\n",
    "\n",
    "prediction_probabilities = rnn_tokenizer.predict(X)\n",
    "y_pred = np.argmax(prediction_probabilities, axis=1) # argmax: return the index of the maximum value # axis=1: for each row\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1664711265345,
     "user": {
      "displayName": "Skillcate AI",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "ZNYiLBT_Pmtg",
    "outputId": "c30606bc-f2a0-425a-e55c-364eea657685"
   },
   "outputs": [],
   "source": [
    "dataset['rnn_tokenizer_prediction'] = y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step\n",
      "[1 1 1 1 0 0 1 1 0 1 0 2 1 0 0 0 1 1 1 1 0 2 1 1 1 0 1 1 0 2 0 1 1 0 0 2 1\n",
      " 0 1 0 2 0 1 0 1 1 0 2 1 1 0 0 0 2 0 1 1 2 0 1 1 0 1 0 1 0 0 1 1 0 0 0 2 0\n",
      " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "prediction_probabilities = rnn_text_vectorization.predict(tf.convert_to_tensor(raw_X)) # convert to a tensor\n",
    "y_pred = np.argmax(prediction_probabilities, axis=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['rnn_textvectorizer_prediction'] = y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>nbc_prediction</th>\n",
       "      <th>rnn_tokenizer_prediction</th>\n",
       "      <th>rnn_textvectorizer_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spend money elsewher</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regular toast bread equal satisfi occasion pat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buffet bellagio far anticip</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drink weak peopl</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>order not correct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>think food flavor textur lack</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>appetit instantli go</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>overal not impress would not go back</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>whole experi underwhelm think well go ninja su...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>have not wast enough life pour salt wound draw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  nbc_prediction  \\\n",
       "0                                spend money elsewher               1   \n",
       "1   regular toast bread equal satisfi occasion pat...               1   \n",
       "2                         buffet bellagio far anticip               1   \n",
       "3                                    drink weak peopl               0   \n",
       "4                                   order not correct               0   \n",
       "..                                                ...             ...   \n",
       "95                      think food flavor textur lack               1   \n",
       "96                               appetit instantli go               0   \n",
       "97               overal not impress would not go back               0   \n",
       "98  whole experi underwhelm think well go ninja su...               1   \n",
       "99  have not wast enough life pour salt wound draw...               0   \n",
       "\n",
       "    rnn_tokenizer_prediction  rnn_textvectorizer_prediction  \n",
       "0                          1                              1  \n",
       "1                          1                              1  \n",
       "2                          0                              1  \n",
       "3                          0                              1  \n",
       "4                          0                              0  \n",
       "..                       ...                            ...  \n",
       "95                         1                              1  \n",
       "96                         0                              1  \n",
       "97                         0                              0  \n",
       "98                         1                              1  \n",
       "99                         0                              0  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight\n",
    "\n",
    "The following facts are already known before hyperparameter tuning:\n",
    "- Each model has been trained with 3 classes from the chosen training dataset.\n",
    "- The currently chosen testing dataset contains only 2 classes.\n",
    "- The class distribution within the chosen training dataset is not uniform.\n",
    "- The average word-count-per-sentence after preprocessing is around 3-4, with a maximum of 19 in the training dataset.\n",
    "- The average word-count-per-sentence after preprocessing is around 65, with a maximum of 472 in the testing dataset.\n",
    "\n",
    "After comparing each model in this notebook, it is even more certain that these models perform poorly on unseen data.\n",
    "\n",
    "Thus, **a hypothesis is made: the models are likely to perform much better when provided with datasets that include random samples and balanced class distributions.**\n",
    "\n",
    "Before proceeding with the next comparison, each model will be retrained with improved datasets which will need to be selected again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This notebook is done by `La Wun Nannda`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyetbEBugtHSndXPIiQz07",
   "collapsed_sections": [],
   "mount_file_id": "1hE3Mnf4mRFiJOvpRszGd1Hk1MAuf2z6J",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
